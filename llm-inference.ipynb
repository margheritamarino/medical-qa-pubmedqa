{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMedQA: A Dataset for Biomedical Research Question Answering\n",
    "\n",
    "Group: TLDR\n",
    "\n",
    "* Federica Maria Laudizi\n",
    "* Francesca Visalli\n",
    "* Margherita Marino\n",
    "* Tomaz Maia Suller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides functions for performing inference on Large Language Models (LLMs) and saving resulting predictions on the PubMedQA labelled dataset for later analysis.\n",
    "\n",
    "THis notebook is kept separate from the others mainly due to environment issues with the vLLM library, as some members of the group were not able to install it. Leaving them separate also gave us more flexibility to run experiments in an independent manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [vLLM](https://vllm.ai) to perform inference based on the models we load from HuggingFace, as that provided the best inference performance when generating text in batches, largely due to its optimisation of the model before execution, its prefix cache -- which caches activations for the system prompt -- and its ability to seamlessly use more than 1 GPU (inference was performed on Kaggle with 2 T4 GPUs).\n",
    "\n",
    "We explored the optimum runtime as well, including with quantisation, but it provided significantly worse performance overall and less control over caching and system resource usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:47:27.334467Z",
     "iopub.status.busy": "2025-05-20T18:47:27.334170Z",
     "iopub.status.idle": "2025-05-20T18:50:53.880474Z",
     "shell.execute_reply": "2025-05-20T18:50:53.879776Z",
     "shell.execute_reply.started": "2025-05-20T18:47:27.334435Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:50:53.883193Z",
     "iopub.status.busy": "2025-05-20T18:50:53.882527Z",
     "iopub.status.idle": "2025-05-20T18:51:20.672785Z",
     "shell.execute_reply": "2025-05-20T18:51:20.672159Z",
     "shell.execute_reply.started": "2025-05-20T18:50:53.883148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from vllm import LLM, SamplingParams\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All prompts start by providing the model a role\n",
    "\n",
    "```\n",
    "You are an expect biomedical researcher who specializes in answering questions about research papers given their abstracts.\n",
    "Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "```\n",
    "\n",
    "Prompts for zero-shot, non-reasoning provided only additional instructions regarding how to answer, and finished with `ANSWER:` to stimulate the model to produce an answer instead of rambling until its token limit was exhausted.\n",
    "\n",
    "```\n",
    "Write ANSWER: and then your answer.\n",
    "Be extremely concise in your answer: questions must be answered only with \"yes\", \"no\" or \"maybe\". Avoid answering \"maybe\" whenever possible, but use it when you are not sure of the answer.\n",
    "```\n",
    "\n",
    "Meanwhile, prompts meant to stimulate reasoning added a request to \"think step-by-step\" at the beginning, as well as an opening `<think>` tag in the end.\n",
    "\n",
    "```\n",
    "Take your time to think step-by-step. After you are done thinking, write ANSWER: [...]\n",
    "```\n",
    "\n",
    "Some prompts provide options for the model to choose from to make decoding answers easier.\n",
    "\n",
    "Finally, RAG prompts incorporate in the `context` field the text and answer of the most similar question in the artificial dataset to the one we provide for inference. Similarity was measured using dot product similarity between embeddings computed by BioSentVec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:51:20.673827Z",
     "iopub.status.busy": "2025-05-20T18:51:20.673556Z",
     "iopub.status.idle": "2025-05-20T18:51:20.679676Z",
     "shell.execute_reply": "2025-05-20T18:51:20.678912Z",
     "shell.execute_reply.started": "2025-05-20T18:51:20.673805Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "COT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Take your time to think step-by-step. After you are done thinking, write ANSWER: and then your answer.\n",
    "Be extremely concise in your answers: questions must be answered only with \"yes\", \"no\" or \"maybe\". Avoid answering \"maybe\" whenever possible, but use it when you are not sure of the answer.\n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "<think>\n",
    "\"\"\".strip()\n",
    "\n",
    "COT_RAG_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Take your time to think step-by-step. After you are done thinking, write ANSWER: and then your answer.\n",
    "Be extremely concise in your answers: questions must be answered only with \"yes\", \"no\" or \"maybe\". Avoid answering \"maybe\" whenever possible, but use it when you are not sure of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "<think>\n",
    "\"\"\".strip()\n",
    "\n",
    "COT_RAG_INSTRUCT_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Take your time to think step-by-step and reason out loud. After you are done thinking, write ANSWER: and then your answer.\n",
    "Be extremely concise in your answers: questions must be answered only with \"yes\", \"no\" or \"maybe\". Avoid answering \"maybe\" whenever possible, but use it when you are not sure of the answer.\n",
    "\n",
    "The following question and its answer may help you in answering your question:\n",
    "\n",
    "{context}\n",
    "\n",
    "Now, answer the following question given the abstract:\n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\"\"\".strip()\n",
    "\n",
    "ZERO_SHOT_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Write ANSWER: and then your answer.\n",
    "Be extremely concise in your answer: questions must be answered only with \"yes\", \"no\" or \"maybe\". Avoid answering \"maybe\" whenever possible, but use it when you are not sure of the answer.\n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\".strip()\n",
    "\n",
    "ZERO_SHOT_MULTIPLE_CHOICE_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Write ANSWER: and then your answer.\n",
    "Be extremely concise in your answer: questions must be answered only with the number of their option. Avoid option 3) (\"maybe\") whenever possible, but use it when you are not sure of the answer.\n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "OPTIONS:\n",
    "1) Yes\n",
    "2) No\n",
    "3) Maybe\n",
    "\n",
    "ANSWER:\n",
    "\"\"\".strip()\n",
    "\n",
    "COT_MULTIPLE_CHOICE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Take your time to think step-by-step. Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Write ANSWER: and then your answer.\n",
    "Be extremely concise in your answer: questions must be answered only with the number of their option. Try to avoid option 3) (\"maybe\"), but use it when you are not sure of the answer. \n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "OPTIONS:\n",
    "1) Yes\n",
    "2) No\n",
    "3) Maybe\n",
    "\n",
    "<think>\n",
    "\"\"\".strip()\n",
    "\n",
    "COT_RAG_MULTIPLE_CHOICE_PROMPT_TEMPLATE = \"\"\"\n",
    "You are an expect biomedical resercher who specializes in answering questions about research papers given their abstracts.\n",
    "Take your time to think step-by-step. Use the provided abstract to answer the question in the end, and complement it with your own domain knowledge and expertise.\n",
    "Write ANSWER: and then your answer.\n",
    "Be extremely concise in your answer: questions must be answered only with the number of their option. Try to avoid option 3) (\"maybe\"), but use it when you are not sure of the answer. \n",
    "\n",
    "{context}\n",
    "\n",
    "{abstract}\n",
    "\n",
    "QUESTION\n",
    "{question}\n",
    "\n",
    "OPTIONS:\n",
    "1) Yes\n",
    "2) No\n",
    "3) Maybe\n",
    "\n",
    "<think>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set experiment parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we select the prompt template, the generative model to load from HuggingFace, and the experiment name to which we save inference results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:51:20.681418Z",
     "iopub.status.busy": "2025-05-20T18:51:20.680822Z",
     "iopub.status.idle": "2025-05-20T18:51:20.706056Z",
     "shell.execute_reply": "2025-05-20T18:51:20.705256Z",
     "shell.execute_reply.started": "2025-05-20T18:51:20.681387Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = COT_RAG_INSTRUCT_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:51:20.707238Z",
     "iopub.status.busy": "2025-05-20T18:51:20.706892Z",
     "iopub.status.idle": "2025-05-20T18:51:20.717913Z",
     "shell.execute_reply": "2025-05-20T18:51:20.717138Z",
     "shell.execute_reply.started": "2025-05-20T18:51:20.707206Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MODEL = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "EXPERIMENT = \"conclusion-instruct-rag-cot-multiple-choice-numbers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:51:20.719080Z",
     "iopub.status.busy": "2025-05-20T18:51:20.718806Z",
     "iopub.status.idle": "2025-05-20T18:51:21.073875Z",
     "shell.execute_reply": "2025-05-20T18:51:21.073156Z",
     "shell.execute_reply.started": "2025-05-20T18:51:20.719061Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds = load_dataset(\"parquet\", data_files={\"train\": \"/kaggle/input/nlp-pubmedqa-rag/labeled.parquet\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:05:23.232484Z",
     "iopub.status.busy": "2025-05-20T19:05:23.231912Z",
     "iopub.status.idle": "2025-05-20T19:05:23.238345Z",
     "shell.execute_reply": "2025-05-20T19:05:23.237610Z",
     "shell.execute_reply.started": "2025-05-20T19:05:23.232458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_question(sample: dict, rag=False) -> dict:\n",
    "    contexts = list(sample[\"context.contexts\"])\n",
    "    labels = list(sample[\"context.labels\"])\n",
    "    long_answer = sample[\"long_answer\"]\n",
    "    labels.append(\"conclusion\")\n",
    "    contexts.append(long_answer)\n",
    "\n",
    "    abstract = \"\\n\\n\".join([\n",
    "        f\"{label.upper()}\\n{context}\"\n",
    "        for label, context in zip(labels, contexts)\n",
    "    ])\n",
    "    if rag:\n",
    "        text = PROMPT_TEMPLATE.format(\n",
    "            abstract=abstract,\n",
    "            question=sample[\"question\"],\n",
    "            context=sample[\"closest_abstract\"] + \"\\n\\n\",\n",
    "        )\n",
    "    else:\n",
    "        text = PROMPT_TEMPLATE.format(\n",
    "            abstract=abstract,\n",
    "            question=sample[\"question\"],\n",
    "        )\n",
    "    return {\n",
    "        \"text\": text\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:05:23.429333Z",
     "iopub.status.busy": "2025-05-20T19:05:23.429124Z",
     "iopub.status.idle": "2025-05-20T19:05:23.692443Z",
     "shell.execute_reply": "2025-05-20T19:05:23.691607Z",
     "shell.execute_reply.started": "2025-05-20T19:05:23.429318Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "text_ds = ds.map(partial(format_question, rag=True)).with_format(columns=[\"text\"], type=\"torch\")\n",
    "text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T19:05:32.932683Z",
     "iopub.status.busy": "2025-05-20T19:05:32.932097Z",
     "iopub.status.idle": "2025-05-20T19:05:32.938854Z",
     "shell.execute_reply": "2025-05-20T19:05:32.938095Z",
     "shell.execute_reply.started": "2025-05-20T19:05:32.932660Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(text_ds[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:01:55.197569Z",
     "iopub.status.busy": "2025-05-17T16:01:55.197052Z",
     "iopub.status.idle": "2025-05-17T16:01:55.219925Z",
     "shell.execute_reply": "2025-05-17T16:01:55.219326Z",
     "shell.execute_reply.started": "2025-05-17T16:01:55.197545Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    text_ds[\"train\"],\n",
    "    batch_size=128,\n",
    ")\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:01:55.220891Z",
     "iopub.status.busy": "2025-05-17T16:01:55.220629Z",
     "iopub.status.idle": "2025-05-17T16:01:55.234108Z",
     "shell.execute_reply": "2025-05-17T16:01:55.233579Z",
     "shell.execute_reply.started": "2025-05-17T16:01:55.220869Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sampling_params = SamplingParams(max_tokens=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-05-17T15:59:26.979471Z",
     "iopub.status.busy": "2025-05-17T15:59:26.979246Z",
     "iopub.status.idle": "2025-05-17T16:01:53.289971Z",
     "shell.execute_reply": "2025-05-17T16:01:53.289265Z",
     "shell.execute_reply.started": "2025-05-17T15:59:26.979443Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=MODEL,\n",
    "    dtype=\"float16\",                      # Mixed precision for T4 Tensor Cores :contentReference[oaicite:12]{index=12}\n",
    "    # For interactive runs that can use 2xT4\n",
    "    tensor_parallel_size=2,               # Shard model across both GPUs :contentReference[oaicite:13]{index=13}\n",
    "    gpu_memory_utilization=0.8,           # Leave 20% buffer to avoid OOM :contentReference[oaicite:14]{index=14}\n",
    "    enforce_eager=False,                  # Use CUDA graphs by default for speed :contentReference[oaicite:15]{index=15}\n",
    "    enable_prefix_caching=True,\n",
    ")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:01:55.235150Z",
     "iopub.status.busy": "2025-05-17T16:01:55.234885Z",
     "iopub.status.idle": "2025-05-17T16:08:19.082039Z",
     "shell.execute_reply": "2025-05-17T16:08:19.081122Z",
     "shell.execute_reply.started": "2025-05-17T16:01:55.235127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_outputs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        generated_text = llm.generate(\n",
    "            batch[\"text\"],\n",
    "            sampling_params=sampling_params,\n",
    "            use_tqdm=True,\n",
    "        )\n",
    "        all_outputs.extend(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:08:19.083216Z",
     "iopub.status.busy": "2025-05-17T16:08:19.082939Z",
     "iopub.status.idle": "2025-05-17T16:08:19.088467Z",
     "shell.execute_reply": "2025-05-17T16:08:19.087674Z",
     "shell.execute_reply.started": "2025-05-17T16:08:19.083189Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(all_outputs[0].prompt)\n",
    "print(\"\\n\", \"-\"*80, \"\\n\")\n",
    "print(all_outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:08:19.089705Z",
     "iopub.status.busy": "2025-05-17T16:08:19.089440Z",
     "iopub.status.idle": "2025-05-17T16:08:19.109093Z",
     "shell.execute_reply": "2025-05-17T16:08:19.108444Z",
     "shell.execute_reply.started": "2025-05-17T16:08:19.089684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "all_text = [\n",
    "    output.outputs[0].text\n",
    "    for output in all_outputs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-17T16:08:19.109905Z",
     "iopub.status.busy": "2025-05-17T16:08:19.109681Z",
     "iopub.status.idle": "2025-05-17T16:08:19.131660Z",
     "shell.execute_reply": "2025-05-17T16:08:19.131058Z",
     "shell.execute_reply.started": "2025-05-17T16:08:19.109889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(f\"{EXPERIMENT}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_text, f)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7471613,
     "sourceId": 11887705,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
